{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "respone= requests.get(\"https://finance.naver.com/news/mainnews.naver\")\n",
    "html = respone.text\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제목, 링크, 내용, 언론사, 날짜 크롤링\n",
    "\n",
    "\n",
    "# 제목\n",
    "title = soup.select_one('.articleSubject > a').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 링크\n",
    "# select()로 하면 제대로 출력 되지 않는다.\n",
    "# select()는 '리스트'로 값이 반환 되는데, .attrs는 리스트에서 href를 찾을 수 없다.\n",
    "link = 'https://finance.naver.com' + soup.select_one('.articleSubject > a').attrs['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내용\n",
    "discription = soup.select_one('.articleSummary').contents[0].strip()\n",
    "discription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 언론사\n",
    "label = soup.select_one('.press').text.strip()\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜\n",
    "date = soup.select_one('.wdate').text.strip()\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 나무태그 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .select()로 전체를 선택한다.\n",
    "# 이후 for문으로 한 개씩 추출한다.\n",
    "# 한 개씩 추출 됐을 때는 select_one()에서 쓸 수 있는 속성 및 메서드를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제목\n",
    "title_result = []\n",
    "tags = soup.select('.block1 .articleSubject')\n",
    "for tag in tags:\n",
    "    title = tag.text.strip()\n",
    "    title_result.append(title)\n",
    "title_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 링크\n",
    "\n",
    "link_result = []\n",
    "link_tags = soup.select('.block1 .articleSubject a')\n",
    "for link_tag in link_tags:\n",
    "    link = 'https://finance.naver.com' + link_tag.attrs['href']\n",
    "    link_result.append(link)\n",
    "link_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내용\n",
    "\n",
    "discription_result = []\n",
    "discription_tags = soup.select('.block1 .articleSummary')\n",
    "for discription_tag in discription_tags:\n",
    "    discription = discription_tag.contents[0].strip()\n",
    "    discription_result.append(discription)\n",
    "discription_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 언론사\n",
    "\n",
    "press_result = []\n",
    "press_tags = soup.select('.block1 .press')\n",
    "for press_tag in press_tags:\n",
    "    press = press_tag.text.strip()\n",
    "    press_result.append(press)\n",
    "press_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜\n",
    "\n",
    "date_result = []\n",
    "date_tags = soup.select('.block1 .wdate')\n",
    "for date_tag in date_tags:\n",
    "    date = date_tag.text\n",
    "    date_result.append(date)\n",
    "date_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3페이지까지 크롤링 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 페이지 번호가 바뀌는 곳에 반복문 요소를 f스트링 형태로 넣기\n",
    "# 요청, 변환, 분석을 반복문 내에 넣어준다. 페이지 크롤링 할 때마다 반복 할 수 있도록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "for i in range(1, 4):\n",
    "    respone= requests.get(f\"https://finance.naver.com/news/mainnews.naver?&page={i}\")\n",
    "    html = respone.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    articles = soup.select('.block1')\n",
    "    for article in articles:\n",
    "        title = article.select_one('.articleSubject').text.strip()\n",
    "        link = 'https://finance.naver.com' + article.select_one('.articleSubject a').attrs['href']\n",
    "        discription = article.select_one('.articleSummary').contents[0].strip()\n",
    "        press = article.select_one('.press').text.strip()\n",
    "        date = article.select_one('.wdate').text.strip()\n",
    "        print(f'제목 : {title}')\n",
    "        print(f'링크 : {link}')\n",
    "        print(f'내용 : {discription}')\n",
    "        print(f'언론사 : {press}')\n",
    "        print(f'날짜 : {date}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 마지막페이지까지 크롤링 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 반복 횟수를 1000까지 해준다. (while을 사용해도 된다)\n",
    "# select_one()을 사용했을때, 요소가 매칭 되지 않으면 None이 반환 되는 특징을 활용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "for i in range(1, 1000):\n",
    "    respone= requests.get(f\"https://finance.naver.com/news/mainnews.naver?&page={i}\")\n",
    "    html = respone.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    articles = soup.select('.block1')\n",
    "    for article in articles:\n",
    "        title = article.select_one('.articleSubject').text.strip()\n",
    "        link = 'https://finance.naver.com' + article.select_one('.articleSubject a').attrs['href']\n",
    "        discription = article.select_one('.articleSummary').contents[0].strip()\n",
    "        press = article.select_one('.press').text.strip()\n",
    "        date = article.select_one('.wdate').text.strip()\n",
    "        print(f'제목 : {title}')\n",
    "        print(f'링크 : {link}')\n",
    "        print(f'내용 : {discription}')\n",
    "        print(f'언론사 : {press}')\n",
    "        print(f'날짜 : {date}')\n",
    "    if soup.select_one('.pgRR') == None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 엑셀 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 총 결과물 담을 리스트 생성\n",
    "# pandas 이용해서 데이터프레임 만들기\n",
    "# to_excel 활용해서 데이터프레임 결과 엑셀에 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for i in range(1, 1000):\n",
    "    respone= requests.get(f\"https://finance.naver.com/news/mainnews.naver?&page={i}\")\n",
    "    html = respone.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    articles = soup.select('.block1')\n",
    "    for article in articles:\n",
    "        title = article.select_one('.articleSubject').text.strip()\n",
    "        link = 'https://finance.naver.com' + article.select_one('.articleSubject a').attrs['href']\n",
    "        discription = article.select_one('.articleSummary').contents[0].strip()\n",
    "        press = article.select_one('.press').text.strip()\n",
    "        date = article.select_one('.wdate').text.strip()\n",
    "        print(f'제목 : {title}')\n",
    "        print(f'링크 : {link}')\n",
    "        print(f'내용 : {discription}')\n",
    "        print(f'언론사 : {press}')\n",
    "        print(f'날짜 : {date}')\n",
    "        data.append([title, link, discription, press, date])\n",
    "\n",
    "    if soup.select_one('.pgRR') == None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns= ['제목', '링크', '내용', '언론사', '날짜'])\n",
    "\n",
    "df.to_excel('네이버 뉴스 크롤링 결과.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
